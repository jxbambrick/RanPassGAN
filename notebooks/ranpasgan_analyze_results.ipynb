{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create a generator/discriminiator to verify passwords are being one_hot_encoded correctly\n",
    "G_OPTIMIZE_LEARNING_RATE = 0.0001\n",
    "D_OPTIMIZE_LEARNING_RATE = 0.00001\n",
    "\n",
    "# Model monitor for storoing genreated passwords, not needed, can be taken out in future\n",
    "FOLDER_PATH = \"training_sessions\"\n",
    "TRAINING_SESSION = None\n",
    "TRAINING_SESSION_PATH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/bambrick/DevCenter/Juypter/PasswordGAN/notebooks/demo\n",
      "Changed current working directory to: /Users/bambrick/DevCenter/Juypter/PasswordGAN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Check if the last directory in the current path is 'notebooks'\n",
    "if 'notebooks' in current_dir.split(os.sep):\n",
    "    # Change the current working directory two levels up\n",
    "    os.chdir('../../')\n",
    "    print(f\"Changed current working directory to: {os.getcwd()}\\n\")\n",
    "else:\n",
    "    print(\"Current directory is not inside 'notebooks', no change needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to preprocess passwords\n",
    "def preprocess_password(password):\n",
    "    # Remove non-numeric characters\n",
    "    password = re.sub(r'\\D', '', password)\n",
    "    # Ensure password is 12 characters long\n",
    "    return password if len(password) == 12 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(password):\n",
    "    # Define the one-hot encoding for each digit\n",
    "    encoding = []\n",
    "    for char in password:\n",
    "        one_hot = [0]*10\n",
    "        one_hot[int(char)] = 1\n",
    "        encoding.extend(one_hot)\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class GeneratorLSTMv2(tf.keras.Model):\n",
    "    def __init__(self, noise_dim=100, **kwargs):\n",
    "        super(GeneratorLSTMv2, self).__init__(**kwargs)\n",
    "        \n",
    "        # Initial dense layer\n",
    "        self.noise_dim = noise_dim\n",
    "        self.dense_1 = tf.keras.layers.Dense(256, activation='relu', input_dim=noise_dim,)\n",
    "        self.batch_norm_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(0.5)\n",
    "        \n",
    "        # LSTM layers for sequence data\n",
    "        self.reshape_1 = tf.keras.layers.Reshape((1, 256))  # Reshape input for LSTM\n",
    "        self.lstm_1 = tf.keras.layers.LSTM(128, return_sequences=True)\n",
    "        self.lstm_2 = tf.keras.layers.LSTM(128)\n",
    "        self.batch_norm_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
    "        \n",
    "        # Output layers\n",
    "        self.dense_out = tf.keras.layers.Dense(12 * 10, activation='softmax')  # Adjusted to 12 characters, 10 classes each (digits 0-9)\n",
    "        self.reshape_out = tf.keras.layers.Reshape((12, 10))  # Reshape for output\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense_1(inputs)\n",
    "        x = self.batch_norm_1(x, training=training)\n",
    "        x = self.dropout_1(x, training=training)\n",
    "        \n",
    "        x = self.reshape_1(x)\n",
    "        x = self.lstm_1(x)\n",
    "        x = self.lstm_2(x)\n",
    "        x = self.batch_norm_2(x, training=training)\n",
    "        x = self.dropout_2(x, training=training)\n",
    "        \n",
    "        x = self.dense_out(x)\n",
    "        x = self.reshape_out(x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'noise_dim': self.noise_dim\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DiscriminatorLSTMv2(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, input_shape=(12, 10), **kwargs):\n",
    "        super(DiscriminatorLSTMv2, self).__init__(**kwargs)\n",
    "        self._input_shape = input_shape  # Save input_shape as an attribute    \n",
    "        self.flatten = tf.keras.layers.Flatten(input_shape=input_shape)\n",
    "        \n",
    "        # Layer 1\n",
    "        self.dense_1 = tf.keras.layers.Dense(256)\n",
    "        self.leaky_relu_1 = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.batch_norm_1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Layer 2\n",
    "        self.dense_2 = tf.keras.layers.Dense(128)\n",
    "        self.leaky_relu_2 = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
    "        self.batch_norm_2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Output layer\n",
    "        self.dense_out = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.flatten(inputs)\n",
    "        \n",
    "        x = self.dense_1(x)\n",
    "        x = self.leaky_relu_1(x)\n",
    "        x = self.dropout_1(x, training=training)\n",
    "        x = self.batch_norm_1(x, training=training)\n",
    "        \n",
    "        x = self.dense_2(x)\n",
    "        x = self.leaky_relu_2(x)\n",
    "        x = self.dropout_2(x, training=training)\n",
    "        x = self.batch_norm_2(x, training=training)\n",
    "        \n",
    "        x = self.dense_out(x)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'input_shape': self._input_shape  # Directly use the input_shape passed during initialization\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "def configure_losses_optimizers():\n",
    "    gen_opt = Adam(learning_rate=G_OPTIMIZE_LEARNING_RATE)\n",
    "    gen_loss = BinaryCrossentropy()\n",
    "    \n",
    "    dis_opt = Adam(learning_rate=D_OPTIMIZE_LEARNING_RATE)\n",
    "    dis_loss = BinaryCrossentropy()\n",
    "\n",
    "    return gen_opt, gen_loss, dis_opt, dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RanPassGAN(tf.keras.Model): #(Model):\n",
    "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
    "        # Pass through args and kwargs to base class \n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for gen and disc\n",
    "        self.generator = generator \n",
    "        self.discriminator = discriminator \n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"generator_config\": self.generator.get_config(),\n",
    "            \"discriminator_config\": self.discriminator.get_config()\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # You might need to modify this depending on how your generator and discriminator are initialized\n",
    "        generator = GeneratorLSTMv2.from_config(config['generator_config'])\n",
    "        discriminator = DiscriminatorLSTMv2.from_config(config['discriminator_config'])\n",
    "        \n",
    "        return cls(generator=generator, discriminator=discriminator)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        generated_passwords = self.generator(inputs, training=training)\n",
    "        return generated_passwords\n",
    "        \n",
    "    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs): \n",
    "        # Compile with base class\n",
    "        super().compile(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for losses and optimizers\n",
    "        self.g_opt = g_opt\n",
    "        self.d_opt = d_opt\n",
    "        self.g_loss = g_loss\n",
    "        self.d_loss = d_loss \n",
    "        \n",
    "    def train_step(self, batch):\n",
    "        # Get the data \n",
    "        real_passwords = batch\n",
    "        batch_size = tf.shape(real_passwords)[0]  # Dynamically get the batch size\n",
    "\n",
    "        # Generate noise for the generator\n",
    "        noise = tf.random.normal([batch_size, 100])\n",
    "\n",
    "        # Generate fake passwords using the generator\n",
    "        fake_passwords = self.generator(noise, training=True)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as d_tape: \n",
    "            # Pass the real and fake passwords to the discriminator model\n",
    "            yhat_real = self.discriminator(real_passwords, training=True) \n",
    "            yhat_fake = self.discriminator(fake_passwords, training=True)\n",
    "            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n",
    "\n",
    "            # Create labels for real and fakes passwords\n",
    "            y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)\n",
    "            \n",
    "            # Calculate loss - BINARYCROSS \n",
    "            total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n",
    "            \n",
    "        # Apply backpropagation - nn learn \n",
    "        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) \n",
    "        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # Train the generator \n",
    "        with tf.GradientTape() as g_tape: \n",
    "            # Generate some new passwords\n",
    "            gen_passwords = self.generator(tf.random.normal((128, 100)), training=True)\n",
    "                                        \n",
    "            # Create the predicted labels\n",
    "            predicted_labels = self.discriminator(gen_passwords, training=False)\n",
    "                                        \n",
    "            # Calculate loss - trick to training to fake out the discriminator\n",
    "            total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels) \n",
    "            \n",
    "        # Apply backprop\n",
    "        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n",
    "        \n",
    "        return {\"d_loss\":total_d_loss, \"g_loss\":total_g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class ModelMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, num_passwords=10, latent_dim=100):\n",
    "        self.num_passwords = num_passwords\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.uniform((self.num_passwords, self.latent_dim))\n",
    "        generated_outputs = self.model.generator(random_latent_vectors)\n",
    "\n",
    "        # Convert the generated softmax outputs into digits\n",
    "        generated_passwords = [self.softmax_to_digit(output) for output in generated_outputs]\n",
    "\n",
    "        results_file_path = os.path.join(TRAINING_SESSION_PATH, 'epoch_training_results.md')\n",
    "\n",
    "        # Save to a file\n",
    "        with open(results_file_path, 'a') as file:\n",
    "            file.write(f\"\\n\\n## Epoch {epoch} Results\\n\")\n",
    "            for idx, password in enumerate(generated_passwords):\n",
    "                password_str = ''.join(map(str, password))\n",
    "                file.write(f\"- Generated password {idx}: {password_str}\\n\")\n",
    "                print(f\"Epoch {epoch}: Generated password {idx}: {password_str}\")\n",
    "\n",
    "    def softmax_to_digit(self, softmax_output):\n",
    "        return np.argmax(softmax_output, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax_to_digit(softmax_output):\n",
    "    return np.argmax(softmax_output, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password 1: 745016455611\n",
      "Password 2: 742011486611\n",
      "Password 3: 985601450661\n",
      "Password 4: 748615157611\n",
      "Password 5: 788611657691\n",
      "Password 6: 748605456611\n",
      "Password 7: 748611657611\n",
      "Password 8: 788601657611\n",
      "Password 9: 742030455611\n",
      "Password 10: 744018450611\n",
      "\n",
      "Total passwords generated: 100\n",
      "Total nunique passwords generated: 75\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Number of passwords to generate\n",
    "num_passwords = 100\n",
    "\n",
    "# Noise dimension that your generator model expects\n",
    "latent_dim = 100  # Example value\n",
    "\n",
    "# Get the loss optimizers\n",
    "gen_opt, gen_loss, dis_opt, dis_loss = configure_losses_optimizers()\n",
    "\n",
    "# Load model with custom objects\n",
    "custom_objects = {\n",
    "    'RanPassGAN': RanPassGAN,\n",
    "    'GeneratorLSTMv2': GeneratorLSTMv2,\n",
    "    'DiscriminatorLSTMv2': DiscriminatorLSTMv2\n",
    "}\n",
    "\n",
    "# Load the generator model\n",
    "model_path = 'resources/models/numeric_only/metal_12digit_pattern4_full_dataset_8_epochs/models/best_model_epoch_0008'\n",
    "loaded_model = load_model(model_path, custom_objects=custom_objects, compile=False)\n",
    "loaded_model.compile(gen_opt, dis_opt, gen_loss, dis_loss)\n",
    "\n",
    "random_latent_vectors = tf.random.uniform((num_passwords, latent_dim))\n",
    "generated_outputs = loaded_model.generator(random_latent_vectors)\n",
    "\n",
    "# Convert the generated softmax outputs into digits\n",
    "generated_passwords = [softmax_to_digit(output) for output in generated_outputs]\n",
    "\n",
    "password_set = set()\n",
    "for password in generated_passwords:\n",
    "    password_str = ''.join(map(str, password))\n",
    "    \n",
    "# Print the generated passwords\n",
    "for i, password in enumerate(generated_passwords):\n",
    "    password_str = ''.join(map(str, password))\n",
    "    password_set.add(password_str)\n",
    "\n",
    "first_ten_passwords = list(password_set)[:10]\n",
    "for i, password in enumerate(first_ten_passwords):\n",
    "    print(f'Password {i + 1}: {password}')\n",
    "\n",
    "print(f\"\\nTotal passwords generated: 100\")\n",
    "print(f\"Total nunique passwords generated: {len(password_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# # The path to the CSV file where passwords will be saved\n",
    "# csv_file_path = 'generated_passwords.csv'\n",
    "\n",
    "# # Write the passwords to a CSV file\n",
    "# with open(csv_file_path, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "    \n",
    "#     # If you want a header\n",
    "#     writer.writerow(['Password'])\n",
    "    \n",
    "#     # Write passwords to the CSV file\n",
    "#     for password in password_set:\n",
    "#         writer.writerow([password])\n",
    "\n",
    "# print(f\"Passwords have been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# pattern = re.compile(r'^.{3}1.{3}2.{3}3$')\n",
    "# matches = []\n",
    "\n",
    "# # Check each password in the set\n",
    "# for password in password_set:\n",
    "#     if pattern.fullmatch(password):\n",
    "#         print(f\"Password '{password}' matches the pattern.\")\n",
    "\n",
    "# if len(matches) == 0:\n",
    "#     print(\"No matches found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Real password: 481180120293 - 0.49539658427238464\n",
      "Generated password: 748611450661 - 0.4849720895290375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 21:41:59.425930: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming your discriminator model path is correct and the model is properly loaded\n",
    "discriminator = loaded_model.discriminator\n",
    "\n",
    "# The real password to test\n",
    "real_password = '481180120293'\n",
    "\n",
    "# Preprocess the real password using one-hot encoding\n",
    "real_password_encoded = one_hot_encode(real_password)\n",
    "\n",
    "generated_password = ''.join(map(str, softmax_to_digit(generated_outputs[0])))\n",
    "generated_password_encoded = one_hot_encode(generated_password)\n",
    "\n",
    "# Convert the encoded passwords to tensors and add a batch dimension\n",
    "real_password_tensor = tf.convert_to_tensor([real_password_encoded])\n",
    "generated_password_tensor = tf.convert_to_tensor([generated_password_encoded])\n",
    "\n",
    "# Use the discriminator to predict the probability of the password being real\n",
    "real_password_pred = discriminator.predict(real_password_tensor)\n",
    "generated_password_pred = discriminator.predict(generated_password_tensor)\n",
    "\n",
    "# Output the predictions\n",
    "print(f\"Real password: {real_password} - {real_password_pred[0][0]}\")\n",
    "print(f\"Generated password: {generated_password} - {generated_password_pred[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_decode(encoded_password):\n",
    "    # Split the encoded password into chunks of 10 (since there are 10 digits)\n",
    "    digits = [encoded_password[i:i + 10] for i in range(0, len(encoded_password), 10)]\n",
    "    # Find the index of the 1 in each chunk, which corresponds to the digit\n",
    "    decoded_password = ''.join(str(digit.index(1)) for digit in digits)\n",
    "    return decoded_password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Results: Pattern 4 Passwords:\n",
      "\n",
      "473142221343 - 0.48973944783210754\n",
      "244169826023 - 0.49124255776405334\n",
      "842167722903 - 0.47002866864204407\n",
      "804127929213 - 0.5008504390716553\n",
      "863180024843 - 0.4716152250766754\n",
      "978178023473 - 0.49830108880996704\n",
      "993112527413 - 0.5067145824432373\n",
      "398198328603 - 0.48522236943244934\n",
      "222163226293 - 0.4824694097042084\n",
      "484119926733 - 0.4944850206375122\n",
      "\n",
      "Pattern 4 Average: 0.4890901690721512\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming your discriminator model path is correct and the model is properly loaded\n",
    "discriminator = loaded_model.discriminator\n",
    "\n",
    "# Read in a csv file containing passwords, skip the header\n",
    "csv_file_path = 'notebooks/demo/sample_digit12_patttern4.csv'\n",
    "sample_passwords = []\n",
    "\n",
    "with open(csv_file_path, mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header\n",
    "    for row in reader:\n",
    "        # remove any whitespace\n",
    "        sample_passwords.append(row[0].strip())\n",
    "\n",
    "encoded_passwords = []\n",
    "for password in sample_passwords:\n",
    "    encoded_passwords.append(one_hot_encode(password))\n",
    "\n",
    "results_encoded_passwords = []\n",
    "for encoded_password in encoded_passwords:\n",
    "    # Convert the encoded passwords to tensors and add a batch dimension\n",
    "    password_tensor = tf.convert_to_tensor([encoded_password])\n",
    "    \n",
    "    # Use the discriminator to predict the probability of the password being real\n",
    "    password_pred = discriminator.predict(password_tensor)\n",
    "\n",
    "    # Output the predictions\n",
    "    decoded_password = one_hot_decode(encoded_password)\n",
    "    results_encoded_passwords.append((decoded_password, password_pred[0][0]))\n",
    "\n",
    "print(f\"Results: Pattern 4 Passwords:\\n\")\n",
    "# limit to 10 results\n",
    "for result in results_encoded_passwords[:10]:\n",
    "    print(f\"{result[0]} - {result[1]}\")\n",
    "\n",
    "average_probability = sum([result[1] for result in results_encoded_passwords]) / len(results_encoded_passwords)\n",
    "print(f\"\\nPattern 4 Average: {average_probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Results: No Pattern Passwords:\n",
      "\n",
      "524438557904 - 0.5114355087280273\n",
      "130001030904 - 0.5134267210960388\n",
      "367446784959 - 0.48637089133262634\n",
      "858332515423 - 0.5034065246582031\n",
      "791824174636 - 0.4870300889015198\n",
      "377985523425 - 0.5030651092529297\n",
      "044808903469 - 0.5099761486053467\n",
      "817084116821 - 0.48423677682876587\n",
      "660298373003 - 0.480099081993103\n",
      "752893529378 - 0.49188610911369324\n",
      "\n",
      "No Pattern: Average: 0.4956339582800865\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming your discriminator model path is correct and the model is properly loaded\n",
    "discriminator = loaded_model.discriminator\n",
    "\n",
    "# Read in a csv file containing passwords, skip the header\n",
    "csv_file_path = 'notebooks/demo/sample_digit12_no_pattern.csv'\n",
    "sample_passwords = []\n",
    "\n",
    "with open(csv_file_path, mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header\n",
    "    for row in reader:\n",
    "        # remove any whitespace\n",
    "        sample_passwords.append(row[0].strip())\n",
    "\n",
    "encoded_passwords = []\n",
    "for password in sample_passwords:\n",
    "    encoded_passwords.append(one_hot_encode(password))\n",
    "\n",
    "results_encoded_passwords = []\n",
    "for encoded_password in encoded_passwords:\n",
    "    # Convert the encoded passwords to tensors and add a batch dimension\n",
    "    password_tensor = tf.convert_to_tensor([encoded_password])\n",
    "    \n",
    "    # Use the discriminator to predict the probability of the password being real\n",
    "    password_pred = discriminator.predict(password_tensor)\n",
    "\n",
    "    # Output the predictions\n",
    "    decoded_password = one_hot_decode(encoded_password)\n",
    "    results_encoded_passwords.append((decoded_password, password_pred[0][0]))\n",
    "\n",
    "print(f\"Results: No Pattern Passwords:\\n\")\n",
    "# limit to 10 results\n",
    "for result in results_encoded_passwords[:10]:\n",
    "    print(f\"{result[0]} - {result[1]}\")\n",
    "\n",
    "average_probability = sum([result[1] for result in results_encoded_passwords]) / len(results_encoded_passwords)\n",
    "print(f\"\\nNo Pattern: Average: {average_probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Pattern 4 Passwords:\n",
      "\n",
      "524438557904 - 0.5114355087280273\n",
      "130001030904 - 0.5134267210960388\n",
      "367446784959 - 0.48637089133262634\n",
      "858332515423 - 0.5034065246582031\n",
      "791824174636 - 0.4870300889015198\n",
      "377985523425 - 0.5030651092529297\n",
      "044808903469 - 0.5099761486053467\n",
      "817084116821 - 0.48423677682876587\n",
      "660298373003 - 0.480099081993103\n",
      "752893529378 - 0.49188610911369324\n",
      "\n",
      "Pattern 4: Average Probability: 0.4956339582800865\n",
      "\n",
      "\n",
      "Results: No Pattern Passwords:\n",
      "\n",
      "524438557904 - 0.5114355087280273\n",
      "130001030904 - 0.5134267210960388\n",
      "367446784959 - 0.48637089133262634\n",
      "858332515423 - 0.5034065246582031\n",
      "791824174636 - 0.4870300889015198\n",
      "377985523425 - 0.5030651092529297\n",
      "044808903469 - 0.5099761486053467\n",
      "817084116821 - 0.48423677682876587\n",
      "660298373003 - 0.480099081993103\n",
      "752893529378 - 0.49188610911369324\n",
      "\n",
      "No Pattern: Average Probability: 0.4956339582800865\n"
     ]
    }
   ],
   "source": [
    "print(f\"Results: Pattern 4 Passwords:\\n\")\n",
    "# limit to 10 results\n",
    "for result in results_encoded_passwords[:10]:\n",
    "    print(f\"{result[0]} - {result[1]}\")\n",
    "\n",
    "average_probability = sum([result[1] for result in results_encoded_passwords]) / len(results_encoded_passwords)\n",
    "print(f\"\\nPattern 4: Average Probability: {average_probability}\")\n",
    "\n",
    "print(f\"\\n\")\n",
    "print(f\"Results: No Pattern Passwords:\\n\")\n",
    "# limit to 10 results\n",
    "for result in results_encoded_passwords[:10]:\n",
    "    print(f\"{result[0]} - {result[1]}\")\n",
    "\n",
    "average_probability = sum([result[1] for result in results_encoded_passwords]) / len(results_encoded_passwords)\n",
    "print(f\"\\nNo Pattern: Average Probability: {average_probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "passwordgan",
   "language": "python",
   "name": "passwordgan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
