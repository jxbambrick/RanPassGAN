{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create a generator/discriminiator to verify passwords are being one_hot_encoded correctly\n",
    "G_OPTIMIZE_LEARNING_RATE = 0.0001\n",
    "D_OPTIMIZE_LEARNING_RATE = 0.00001\n",
    "\n",
    "# Model monitor for storoing genreated passwords, not needed, can be taken out in future\n",
    "FOLDER_PATH = \"training_sessions\"\n",
    "TRAINING_SESSION = None\n",
    "TRAINING_SESSION_PATH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/bambrick/DevCenter/Juypter/PasswordGAN/notebooks/demo\n",
      "Changed current working directory to: /Users/bambrick/DevCenter/Juypter/PasswordGAN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Check if the last directory in the current path is 'notebooks'\n",
    "if 'notebooks' in current_dir.split(os.sep):\n",
    "    # Change the current working directory two levels up\n",
    "    os.chdir('../../')\n",
    "    print(f\"Changed current working directory to: {os.getcwd()}\\n\")\n",
    "else:\n",
    "    print(\"Current directory is not inside 'notebooks', no change needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to preprocess passwords\n",
    "def preprocess_password(password):\n",
    "    # Remove non-numeric characters\n",
    "    password = re.sub(r'\\D', '', password)\n",
    "    # Ensure password is 12 characters long\n",
    "    return password if len(password) == 12 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(password):\n",
    "    # Define the one-hot encoding for each digit\n",
    "    encoding = []\n",
    "    for char in password:\n",
    "        one_hot = [0]*10\n",
    "        one_hot[int(char)] = 1\n",
    "        encoding.extend(one_hot)\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class GeneratorLSTMv2(tf.keras.Model):\n",
    "    def __init__(self, noise_dim=100, **kwargs):\n",
    "        super(GeneratorLSTMv2, self).__init__(**kwargs)\n",
    "        \n",
    "        # Initial dense layer\n",
    "        self.noise_dim = noise_dim\n",
    "        self.dense_1 = tf.keras.layers.Dense(256, activation='relu', input_dim=noise_dim,)\n",
    "        self.batch_norm_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(0.5)\n",
    "        \n",
    "        # LSTM layers for sequence data\n",
    "        self.reshape_1 = tf.keras.layers.Reshape((1, 256))  # Reshape input for LSTM\n",
    "        self.lstm_1 = tf.keras.layers.LSTM(128, return_sequences=True)\n",
    "        self.lstm_2 = tf.keras.layers.LSTM(128)\n",
    "        self.batch_norm_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
    "        \n",
    "        # Output layers\n",
    "        self.dense_out = tf.keras.layers.Dense(12 * 10, activation='softmax')  # Adjusted to 12 characters, 10 classes each (digits 0-9)\n",
    "        self.reshape_out = tf.keras.layers.Reshape((12, 10))  # Reshape for output\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense_1(inputs)\n",
    "        x = self.batch_norm_1(x, training=training)\n",
    "        x = self.dropout_1(x, training=training)\n",
    "        \n",
    "        x = self.reshape_1(x)\n",
    "        x = self.lstm_1(x)\n",
    "        x = self.lstm_2(x)\n",
    "        x = self.batch_norm_2(x, training=training)\n",
    "        x = self.dropout_2(x, training=training)\n",
    "        \n",
    "        x = self.dense_out(x)\n",
    "        x = self.reshape_out(x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'noise_dim': self.noise_dim\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DiscriminatorLSTMv2(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, input_shape=(12, 10), **kwargs):\n",
    "        super(DiscriminatorLSTMv2, self).__init__(**kwargs)\n",
    "        self._input_shape = input_shape  # Save input_shape as an attribute    \n",
    "        self.flatten = tf.keras.layers.Flatten(input_shape=input_shape)\n",
    "        \n",
    "        # Layer 1\n",
    "        self.dense_1 = tf.keras.layers.Dense(256)\n",
    "        self.leaky_relu_1 = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.batch_norm_1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Layer 2\n",
    "        self.dense_2 = tf.keras.layers.Dense(128)\n",
    "        self.leaky_relu_2 = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
    "        self.batch_norm_2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Output layer\n",
    "        self.dense_out = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.flatten(inputs)\n",
    "        \n",
    "        x = self.dense_1(x)\n",
    "        x = self.leaky_relu_1(x)\n",
    "        x = self.dropout_1(x, training=training)\n",
    "        x = self.batch_norm_1(x, training=training)\n",
    "        \n",
    "        x = self.dense_2(x)\n",
    "        x = self.leaky_relu_2(x)\n",
    "        x = self.dropout_2(x, training=training)\n",
    "        x = self.batch_norm_2(x, training=training)\n",
    "        \n",
    "        x = self.dense_out(x)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'input_shape': self._input_shape  # Directly use the input_shape passed during initialization\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "def configure_losses_optimizers():\n",
    "    gen_opt = Adam(learning_rate=G_OPTIMIZE_LEARNING_RATE)\n",
    "    gen_loss = BinaryCrossentropy()\n",
    "    \n",
    "    dis_opt = Adam(learning_rate=D_OPTIMIZE_LEARNING_RATE)\n",
    "    dis_loss = BinaryCrossentropy()\n",
    "\n",
    "    return gen_opt, gen_loss, dis_opt, dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RanPassGAN(tf.keras.Model): #(Model):\n",
    "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
    "        # Pass through args and kwargs to base class \n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for gen and disc\n",
    "        self.generator = generator \n",
    "        self.discriminator = discriminator \n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"generator_config\": self.generator.get_config(),\n",
    "            \"discriminator_config\": self.discriminator.get_config()\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # You might need to modify this depending on how your generator and discriminator are initialized\n",
    "        generator = GeneratorLSTMv2.from_config(config['generator_config'])\n",
    "        discriminator = DiscriminatorLSTMv2.from_config(config['discriminator_config'])\n",
    "        \n",
    "        return cls(generator=generator, discriminator=discriminator)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        generated_passwords = self.generator(inputs, training=training)\n",
    "        return generated_passwords\n",
    "        \n",
    "    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs): \n",
    "        # Compile with base class\n",
    "        super().compile(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for losses and optimizers\n",
    "        self.g_opt = g_opt\n",
    "        self.d_opt = d_opt\n",
    "        self.g_loss = g_loss\n",
    "        self.d_loss = d_loss \n",
    "        \n",
    "    def train_step(self, batch):\n",
    "        # Get the data \n",
    "        real_passwords = batch\n",
    "        batch_size = tf.shape(real_passwords)[0]  # Dynamically get the batch size\n",
    "\n",
    "        # Generate noise for the generator\n",
    "        noise = tf.random.normal([batch_size, 100])\n",
    "\n",
    "        # Generate fake passwords using the generator\n",
    "        fake_passwords = self.generator(noise, training=True)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as d_tape: \n",
    "            # Pass the real and fake passwords to the discriminator model\n",
    "            yhat_real = self.discriminator(real_passwords, training=True) \n",
    "            yhat_fake = self.discriminator(fake_passwords, training=True)\n",
    "            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n",
    "\n",
    "            # Create labels for real and fakes passwords\n",
    "            y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)\n",
    "            \n",
    "            # Calculate loss - BINARYCROSS \n",
    "            total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n",
    "            \n",
    "        # Apply backpropagation - nn learn \n",
    "        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) \n",
    "        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # Train the generator \n",
    "        with tf.GradientTape() as g_tape: \n",
    "            # Generate some new passwords\n",
    "            gen_passwords = self.generator(tf.random.normal((128, 100)), training=True)\n",
    "                                        \n",
    "            # Create the predicted labels\n",
    "            predicted_labels = self.discriminator(gen_passwords, training=False)\n",
    "                                        \n",
    "            # Calculate loss - trick to training to fake out the discriminator\n",
    "            total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels) \n",
    "            \n",
    "        # Apply backprop\n",
    "        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n",
    "        \n",
    "        return {\"d_loss\":total_d_loss, \"g_loss\":total_g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class ModelMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, num_passwords=10, latent_dim=100):\n",
    "        self.num_passwords = num_passwords\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.uniform((self.num_passwords, self.latent_dim))\n",
    "        generated_outputs = self.model.generator(random_latent_vectors)\n",
    "\n",
    "        # Convert the generated softmax outputs into digits\n",
    "        generated_passwords = [self.softmax_to_digit(output) for output in generated_outputs]\n",
    "\n",
    "        results_file_path = os.path.join(TRAINING_SESSION_PATH, 'epoch_training_results.md')\n",
    "\n",
    "        # Save to a file\n",
    "        with open(results_file_path, 'a') as file:\n",
    "            file.write(f\"\\n\\n## Epoch {epoch} Results\\n\")\n",
    "            for idx, password in enumerate(generated_passwords):\n",
    "                password_str = ''.join(map(str, password))\n",
    "                file.write(f\"- Generated password {idx}: {password_str}\\n\")\n",
    "                print(f\"Epoch {epoch}: Generated password {idx}: {password_str}\")\n",
    "\n",
    "    def softmax_to_digit(self, softmax_output):\n",
    "        return np.argmax(softmax_output, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax_to_digit(softmax_output):\n",
    "    return np.argmax(softmax_output, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 22:34:10.167516: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2023-11-11 22:34:10.167537: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2023-11-11 22:34:10.167541: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2023-11-11 22:34:10.167575: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-11 22:34:10.167590: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total nunique passwords generated: 77\n",
      "Password 1: 745612450611\n",
      "Password 2: 748601757611\n",
      "Password 3: 741615452613\n",
      "Password 4: 788611455611\n",
      "Password 5: 745416486611\n",
      "Password 6: 745011450661\n",
      "Password 7: 741611456611\n",
      "Password 8: 788611456611\n",
      "Password 9: 748615150611\n",
      "Password 10: 748005450611\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Number of passwords to generate\n",
    "num_passwords = 10000\n",
    "\n",
    "# Noise dimension that your generator model expects\n",
    "latent_dim = 100  # Example value\n",
    "\n",
    "# Get the loss optimizers\n",
    "gen_opt, gen_loss, dis_opt, dis_loss = configure_losses_optimizers()\n",
    "\n",
    "# Load model with custom objects\n",
    "custom_objects = {\n",
    "    'RanPassGAN': RanPassGAN,\n",
    "    'GeneratorLSTMv2': GeneratorLSTMv2,\n",
    "    'DiscriminatorLSTMv2': DiscriminatorLSTMv2\n",
    "}\n",
    "\n",
    "# Load the generator model\n",
    "model_path = 'resources/models/numeric_only/metal_12digit_pattern4_full_dataset_8_epochs/models/best_model_epoch_0008'\n",
    "loaded_model = load_model(model_path, custom_objects=custom_objects, compile=False)\n",
    "loaded_model.compile(gen_opt, dis_opt, gen_loss, dis_loss)\n",
    "\n",
    "random_latent_vectors = tf.random.uniform((num_passwords, latent_dim))\n",
    "generated_outputs = loaded_model.generator(random_latent_vectors)\n",
    "\n",
    "# Convert the generated softmax outputs into digits\n",
    "generated_passwords = [softmax_to_digit(output) for output in generated_outputs]\n",
    "\n",
    "password_set = set()\n",
    "for password in generated_passwords:\n",
    "    password_str = ''.join(map(str, password))\n",
    "    \n",
    "# Print the generated passwords\n",
    "for i, password in enumerate(generated_passwords):\n",
    "    password_str = ''.join(map(str, password))\n",
    "    password_set.add(password_str)\n",
    "\n",
    "print(f\"\\nTotal nunique passwords generated: {len(password_set)}\")\n",
    "first_ten_passwords = list(password_set)[:10]\n",
    "for i, password in enumerate(first_ten_passwords):\n",
    "    print(f'Password {i + 1}: {password}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passwords have been written to generated_passwords.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# The path to the CSV file where passwords will be saved\n",
    "csv_file_path = 'generated_passwords.csv'\n",
    "\n",
    "# Write the passwords to a CSV file\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # If you want a header\n",
    "    writer.writerow(['Password'])\n",
    "    \n",
    "    # Write passwords to the CSV file\n",
    "    for password in password_set:\n",
    "        writer.writerow([password])\n",
    "\n",
    "print(f\"Passwords have been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches found\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile(r'^.{3}1.{3}2.{3}3$')\n",
    "matches = []\n",
    "\n",
    "# Check each password in the set\n",
    "for password in password_set:\n",
    "    if pattern.fullmatch(password):\n",
    "        print(f\"Password '{password}' matches the pattern.\")\n",
    "\n",
    "if len(matches) == 0:\n",
    "    print(\"No matches found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Real password: 111122223333 - 0.4760373830795288\n",
      "Generated password: 748611456611 - 0.49195027351379395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 22:34:12.383876: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming your discriminator model path is correct and the model is properly loaded\n",
    "discriminator = loaded_model.discriminator\n",
    "\n",
    "# The real password to test\n",
    "real_password = '111122223333'\n",
    "\n",
    "# Preprocess the real password using one-hot encoding\n",
    "real_password_encoded = one_hot_encode(real_password)\n",
    "\n",
    "generated_password = ''.join(map(str, softmax_to_digit(generated_outputs[0])))\n",
    "generated_password_encoded = one_hot_encode(generated_password)\n",
    "\n",
    "# Convert the encoded passwords to tensors and add a batch dimension\n",
    "real_password_tensor = tf.convert_to_tensor([real_password_encoded])\n",
    "generated_password_tensor = tf.convert_to_tensor([generated_password_encoded])\n",
    "\n",
    "# Use the discriminator to predict the probability of the password being real\n",
    "real_password_pred = discriminator.predict(real_password_tensor)\n",
    "generated_password_pred = discriminator.predict(generated_password_tensor)\n",
    "\n",
    "# Output the predictions\n",
    "print(f\"Real password: {real_password} - {real_password_pred[0][0]}\")\n",
    "print(f\"Generated password: {generated_password} - {generated_password_pred[0][0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "passwordgan",
   "language": "python",
   "name": "passwordgan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
