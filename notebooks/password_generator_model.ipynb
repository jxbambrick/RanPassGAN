{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create a generator/discriminiator to verify passwords are being one_hot_encoded correctly\n",
    "G_OPTIMIZE_LEARNING_RATE = 0.0001\n",
    "D_OPTIMIZE_LEARNING_RATE = 0.00001\n",
    "\n",
    "# Model monitor for storoing genreated passwords, not needed, can be taken out in future\n",
    "FOLDER_PATH = \"training_sessions\"\n",
    "TRAINING_SESSION = None\n",
    "TRAINING_SESSION_PATH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/bambrick/DevCenter/Juypter/PasswordGAN/notebooks/demo\n",
      "Changed current working directory to: /Users/bambrick/DevCenter/Juypter/PasswordGAN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Check if the last directory in the current path is 'notebooks'\n",
    "if 'notebooks' in current_dir.split(os.sep):\n",
    "    # Change the current working directory two levels up\n",
    "    os.chdir('../../')\n",
    "    print(f\"Changed current working directory to: {os.getcwd()}\\n\")\n",
    "else:\n",
    "    print(\"Current directory is not inside 'notebooks', no change needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(password):\n",
    "    # Define the one-hot encoding for each digit\n",
    "    encoding = []\n",
    "    for char in password:\n",
    "        one_hot = [0]*10\n",
    "        one_hot[int(char)] = 1\n",
    "        encoding.extend(one_hot)\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class GeneratorLSTMv2(tf.keras.Model):\n",
    "    def __init__(self, noise_dim=100, **kwargs):\n",
    "        super(GeneratorLSTMv2, self).__init__(**kwargs)\n",
    "        \n",
    "        # Initial dense layer\n",
    "        self.noise_dim = noise_dim\n",
    "        self.dense_1 = tf.keras.layers.Dense(256, activation='relu', input_dim=noise_dim,)\n",
    "        self.batch_norm_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(0.5)\n",
    "        \n",
    "        # LSTM layers for sequence data\n",
    "        self.reshape_1 = tf.keras.layers.Reshape((1, 256))  # Reshape input for LSTM\n",
    "        self.lstm_1 = tf.keras.layers.LSTM(128, return_sequences=True)\n",
    "        self.lstm_2 = tf.keras.layers.LSTM(128)\n",
    "        self.batch_norm_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
    "        \n",
    "        # Output layers\n",
    "        self.dense_out = tf.keras.layers.Dense(12 * 10, activation='softmax')  # Adjusted to 12 characters, 10 classes each (digits 0-9)\n",
    "        self.reshape_out = tf.keras.layers.Reshape((12, 10))  # Reshape for output\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense_1(inputs)\n",
    "        x = self.batch_norm_1(x, training=training)\n",
    "        x = self.dropout_1(x, training=training)\n",
    "        \n",
    "        x = self.reshape_1(x)\n",
    "        x = self.lstm_1(x)\n",
    "        x = self.lstm_2(x)\n",
    "        x = self.batch_norm_2(x, training=training)\n",
    "        x = self.dropout_2(x, training=training)\n",
    "        \n",
    "        x = self.dense_out(x)\n",
    "        x = self.reshape_out(x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'noise_dim': self.noise_dim\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DiscriminatorLSTMv2(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, input_shape=(12, 10), **kwargs):\n",
    "        super(DiscriminatorLSTMv2, self).__init__(**kwargs)\n",
    "        self._input_shape = input_shape  # Save input_shape as an attribute    \n",
    "        self.flatten = tf.keras.layers.Flatten(input_shape=input_shape)\n",
    "        \n",
    "        # Layer 1\n",
    "        self.dense_1 = tf.keras.layers.Dense(256)\n",
    "        self.leaky_relu_1 = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.batch_norm_1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Layer 2\n",
    "        self.dense_2 = tf.keras.layers.Dense(128)\n",
    "        self.leaky_relu_2 = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
    "        self.batch_norm_2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Output layer\n",
    "        self.dense_out = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.flatten(inputs)\n",
    "        \n",
    "        x = self.dense_1(x)\n",
    "        x = self.leaky_relu_1(x)\n",
    "        x = self.dropout_1(x, training=training)\n",
    "        x = self.batch_norm_1(x, training=training)\n",
    "        \n",
    "        x = self.dense_2(x)\n",
    "        x = self.leaky_relu_2(x)\n",
    "        x = self.dropout_2(x, training=training)\n",
    "        x = self.batch_norm_2(x, training=training)\n",
    "        \n",
    "        x = self.dense_out(x)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'input_shape': self._input_shape  # Directly use the input_shape passed during initialization\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "def configure_losses_optimizers():\n",
    "    gen_opt = Adam(learning_rate=G_OPTIMIZE_LEARNING_RATE)\n",
    "    gen_loss = BinaryCrossentropy()\n",
    "    \n",
    "    dis_opt = Adam(learning_rate=D_OPTIMIZE_LEARNING_RATE)\n",
    "    dis_loss = BinaryCrossentropy()\n",
    "\n",
    "    return gen_opt, gen_loss, dis_opt, dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RanPassGAN(tf.keras.Model): #(Model):\n",
    "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
    "        # Pass through args and kwargs to base class \n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for gen and disc\n",
    "        self.generator = generator \n",
    "        self.discriminator = discriminator \n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"generator_config\": self.generator.get_config(),\n",
    "            \"discriminator_config\": self.discriminator.get_config()\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        generator = GeneratorLSTMv2.from_config(config['generator_config'])\n",
    "        discriminator = DiscriminatorLSTMv2.from_config(config['discriminator_config'])\n",
    "        \n",
    "        return cls(generator=generator, discriminator=discriminator)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        generated_passwords = self.generator(inputs, training=training)\n",
    "        return generated_passwords\n",
    "        \n",
    "    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs): \n",
    "        # Compile with base class\n",
    "        super().compile(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for losses and optimizers\n",
    "        self.g_opt = g_opt\n",
    "        self.d_opt = d_opt\n",
    "        self.g_loss = g_loss\n",
    "        self.d_loss = d_loss \n",
    "        \n",
    "    def train_step(self, batch):\n",
    "        # Get the data \n",
    "        real_passwords = batch\n",
    "        batch_size = tf.shape(real_passwords)[0]  # Dynamically get the batch size\n",
    "\n",
    "        # Generate noise for the generator\n",
    "        noise = tf.random.normal([batch_size, 100])\n",
    "\n",
    "        # Generate fake passwords using the generator\n",
    "        fake_passwords = self.generator(noise, training=True)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as d_tape: \n",
    "            # Pass the real and fake passwords to the discriminator model\n",
    "            yhat_real = self.discriminator(real_passwords, training=True) \n",
    "            yhat_fake = self.discriminator(fake_passwords, training=True)\n",
    "            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n",
    "\n",
    "            # Create labels for real and fakes passwords\n",
    "            y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)\n",
    "            \n",
    "            # Calculate loss - BINARYCROSS \n",
    "            total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n",
    "            \n",
    "        # Apply backpropagation\n",
    "        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) \n",
    "        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # Train the generator \n",
    "        with tf.GradientTape() as g_tape: \n",
    "            # Generate some new passwords\n",
    "            gen_passwords = self.generator(tf.random.normal((128, 100)), training=True)\n",
    "                                        \n",
    "            # Create the predicted labels\n",
    "            predicted_labels = self.discriminator(gen_passwords, training=False)\n",
    "                                        \n",
    "            # Calculate loss - trick to training to fake out the discriminator\n",
    "            total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels) \n",
    "            \n",
    "        # Apply backprop\n",
    "        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n",
    "        \n",
    "        return {\"d_loss\":total_d_loss, \"g_loss\":total_g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class ModelMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, num_passwords=10, latent_dim=100):\n",
    "        self.num_passwords = num_passwords\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.uniform((self.num_passwords, self.latent_dim))\n",
    "        generated_outputs = self.model.generator(random_latent_vectors)\n",
    "\n",
    "        # Convert the generated softmax outputs into digits\n",
    "        generated_passwords = [self.softmax_to_digit(output) for output in generated_outputs]\n",
    "\n",
    "        results_file_path = os.path.join(TRAINING_SESSION_PATH, 'epoch_training_results.md')\n",
    "\n",
    "        # Save to a file\n",
    "        with open(results_file_path, 'a') as file:\n",
    "            file.write(f\"\\n\\n## Epoch {epoch} Results\\n\")\n",
    "            for idx, password in enumerate(generated_passwords):\n",
    "                password_str = ''.join(map(str, password))\n",
    "                file.write(f\"- Generated password {idx}: {password_str}\\n\")\n",
    "                print(f\"Epoch {epoch}: Generated password {idx}: {password_str}\")\n",
    "\n",
    "    def softmax_to_digit(self, softmax_output):\n",
    "        return np.argmax(softmax_output, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax_to_digit(softmax_output):\n",
    "    return np.argmax(softmax_output, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 22:28:44.700371: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2023-11-11 22:28:44.700399: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2023-11-11 22:28:44.700403: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2023-11-11 22:28:44.700436: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-11 22:28:44.700452: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Number of passwords to generate\n",
    "num_passwords = 100\n",
    "\n",
    "# Noise dimension that your generator model expects\n",
    "latent_dim = 100  # Example value\n",
    "\n",
    "# Get the loss optimizers\n",
    "gen_opt, gen_loss, dis_opt, dis_loss = configure_losses_optimizers()\n",
    "\n",
    "# Load model with custom objects\n",
    "custom_objects = {\n",
    "    'RanPassGAN': RanPassGAN,\n",
    "    'GeneratorLSTMv2': GeneratorLSTMv2,\n",
    "    'DiscriminatorLSTMv2': DiscriminatorLSTMv2\n",
    "}\n",
    "\n",
    "# Load the generator model\n",
    "model_path = 'resources/models/numeric_only/metal_12digit_pattern4_full_dataset_8_epochs/models/best_model_epoch_0008'\n",
    "loaded_model = load_model(model_path, custom_objects=custom_objects, compile=False)\n",
    "loaded_model.compile(gen_opt, dis_opt, gen_loss, dis_loss)\n",
    "\n",
    "discriminator = loaded_model.discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the password blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file(file_path):\n",
    "    \"\"\"\n",
    "    This function loads a text file into a list, where each line is an element in the list.\n",
    "    \n",
    "    :param file_path: str, the path to the text file to be loaded\n",
    "    :return: list of strings, where each string is a line from the file\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = [line.strip() for line in file.readlines()]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def password_in_list(password_to_check):\n",
    "\n",
    "    password_blackist_path = 'resources/data/password_blacklist/10-million-password-list-top-10000.txt'\n",
    "    password_list = load_text_file(password_blackist_path)\n",
    "\n",
    "    return password_to_check in password_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate Secure Password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from password_generator import PasswordGenerator\n",
    "\n",
    "pw_generator = PasswordGenerator()\n",
    "pw_generator.secure = True\n",
    "pw_generator.use_upper = False\n",
    "pw_generator.use_lower = False\n",
    "pw_generator.use_digits = True\n",
    "pw_generator.use_special = False\n",
    "pw_generator.avoid_ambiguous = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#  Check if a password is considered real by the discriminator model.\n",
    "def check_password_against_model(discriminator, password, certanty_level=0.45):\n",
    "\n",
    "    # Preprocess the real password using one-hot encoding\n",
    "    password_encoded = one_hot_encode(password)\n",
    "\n",
    "    # Convert the encoded passwords to tensors and add a batch dimension\n",
    "    password_tensor = tf.convert_to_tensor([password_encoded], dtype=tf.float32)\n",
    "\n",
    "    # Use the discriminator to predict the probability of the password being real\n",
    "    password_pred = discriminator.predict(password_tensor)\n",
    "\n",
    "    # Extract the certainty level of the prediction\n",
    "    password_certainty = password_pred[0][0]\n",
    "\n",
    "    # Return True if the password certainty is greater than the certainty level threshold\n",
    "    return password_certainty > certanty_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Generate a secure password using the generator and check if it is considered real by the discriminator\n",
    "def generate_secure_password(discriminator, certanty_level=0.45):\n",
    "    while True:\n",
    "        # Generate a new password\n",
    "        password = pw_generator.generate()\n",
    "\n",
    "        # Check if the password is in the blacklist\n",
    "        if password_in_list(password):\n",
    "            print(\"Password is in blacklist, generating a new one...\")\n",
    "            continue  # Skip to the next iteration and generate a new password\n",
    "            \n",
    "        # Preprocess the real password using one-hot encoding\n",
    "        password_encoded = one_hot_encode(password)\n",
    "\n",
    "        # Convert the encoded passwords to tensors and add a batch dimension\n",
    "        password_tensor = tf.convert_to_tensor([password_encoded])\n",
    "\n",
    "        # Use the discriminator to predict the probability of the password being real\n",
    "        password_pred = discriminator.predict(password_tensor)\n",
    "\n",
    "        password_certainty = password_pred[0][0]\n",
    "\n",
    "        # Check if the password is considered real by the discriminator\n",
    "        if password_certainty > certanty_level:\n",
    "            # The password is not in the blacklist and is considered real by the discriminator\n",
    "            return password, password_certainty\n",
    "        else:\n",
    "            print(\"Password didn't pass the discriminator check, generating a new one...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 252ms/step\n",
      "Generated password: 822127523283 - 0.47853705286979675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 22:28:46.317156: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "secure_password, password_certainty = generate_secure_password(discriminator)\n",
    "print(f\"Generated password: {secure_password} - {password_certainty}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "passwordgan",
   "language": "python",
   "name": "passwordgan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
